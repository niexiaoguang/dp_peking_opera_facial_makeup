{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import io\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras import Sequential, Input, Model\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing import image\n",
    "from scipy.misc import imread, imsave\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# K.set_image_dim_ordering('tf')\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "def write_log(callback, name, loss, batch_no):\n",
    "    \"\"\"\n",
    "    Write training summary to TensorBoard\n",
    "    \"\"\"\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = loss\n",
    "    summary_value.tag = name\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()\n",
    "\n",
    "    \n",
    "    \n",
    "def build_generator():\n",
    "    gen_model = Sequential()\n",
    "\n",
    "    gen_model.add(Dense(input_dim=100, output_dim=2048))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "\n",
    "    gen_model.add(Dense(256 * 8 * 8))\n",
    "    gen_model.add(BatchNormalization())\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "    # 8x8 \n",
    "    gen_model.add(Reshape((8, 8, 256), input_shape=(256 * 8 * 8,)))\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "    # 16x16\n",
    "    gen_model.add(Conv2D(128, (5, 5), padding='same'))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "    # 32x32\n",
    "    gen_model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "\n",
    "    # 64x64\n",
    "    gen_model.add(Conv2D(32, (5, 5), padding='same'))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "\n",
    "    # 128x128\n",
    "    gen_model.add(Conv2D(3, (5, 5), padding='same'))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "    return gen_model\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "\n",
    "\n",
    "    dis_model = Sequential()\n",
    "\n",
    "    w = 128\n",
    "    h = 128\n",
    "    dis_model.add(\n",
    "        Conv2D(128, (5, 5),\n",
    "               padding='same',\n",
    "               input_shape=(w, h, 3))\n",
    "    )\n",
    "\n",
    "\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "    dis_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 64x64\n",
    "    dis_model.add(Conv2D(256, (3, 3)))\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "    dis_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 32x32\n",
    "    dis_model.add(Conv2D(512, (3, 3)))\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "    dis_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 16x16\n",
    "    dis_model.add(Flatten())\n",
    "    dis_model.add(Dense(1024))\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    dis_model.add(Dense(1))\n",
    "    dis_model.add(Activation('sigmoid'))\n",
    "\n",
    "    return dis_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rawImagesPath = \"rawimages\"\n",
    "readyImagesPath = \"readyimages\"\n",
    "\n",
    "def pre_process_img(input_path, output_path):\n",
    "\n",
    "    w = 128\n",
    "    h = 128\n",
    "    files = os.listdir(rawImagesPath)\n",
    "    os.chdir(input_path)\n",
    "    if(not os.path.exists(output_path)):\n",
    "        os.makedirs(output_path)\n",
    "    for file in files:\n",
    "        if(os.path.isfile(file) & file.endswith('.jpg')):\n",
    "            img = Image.open(file)\n",
    "            img = img.resize((w, h), Image.ANTIALIAS)\n",
    "            img.save(os.path.join(readyImagesPath,file))\n",
    "\n",
    "\n",
    "\n",
    "def post_process_img(img):\n",
    "    res = img\n",
    "    return res\n",
    "\n",
    "# def train():\n",
    "#     batch_size = 64\n",
    "#     z_shape = 100\n",
    "#     epochs = 10000\n",
    "#     dis_learning_rate = 0.0005\n",
    "#     gen_learning_rate = 0.0004\n",
    "#     dis_momentum = 0.9\n",
    "#     gen_momentum = 0.9\n",
    "#     dis_nesterov = True\n",
    "#     gen_nesterov = True\n",
    "\n",
    "\n",
    "#     # Loading images\n",
    "#     all_images = []\n",
    "#     for index, filename in enumerate(glob.glob(readyImagesPath + '/*.jpg')): \n",
    "#         image = imread(filename, flatten=False, mode='RGB')\n",
    "#         all_images.append(image)\n",
    "\n",
    "#     # Convert to Numpy ndarray\n",
    "#     X = np.array(all_images)\n",
    "#     X = (X - 127.5) / 127.5\n",
    "\n",
    "\n",
    "#     # Define optimizers\n",
    "#     dis_optimizer = SGD(lr=dis_learning_rate, momentum=dis_momentum, nesterov=dis_nesterov)\n",
    "#     gen_optimizer = SGD(lr=gen_learning_rate, momentum=gen_momentum, nesterov=gen_nesterov)\n",
    "\n",
    "\n",
    "#     gen_model = build_generator()\n",
    "#     gen_model.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "\n",
    "\n",
    "#     dis_model = build_discriminator()\n",
    "#     dis_model.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "#     adversarial_model = Sequential()\n",
    "#     adversarial_model.add(gen_model)\n",
    "#     dis_model.trainable = False\n",
    "#     adversarial_model.add(dis_model)\n",
    "\n",
    "\n",
    "#     adversarial_model.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "\n",
    "#     tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time.time()), write_images=True, write_grads=True,\n",
    "#                                   write_graph=True)\n",
    "\n",
    "#     tensorboard.set_model(gen_model)\n",
    "#     tensorboard.set_model(dis_model)\n",
    "\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         print(\"Epoch is\", epoch)\n",
    "#         number_of_batches = int(X.shape[0] / batch_size)\n",
    "#         print(\"Number of batches\", number_of_batches)\n",
    "#         for index in range(number_of_batches):\n",
    "\n",
    "\n",
    "#             z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "\n",
    "\n",
    "#             image_batch = X[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "#             generated_images = gen_model.predict_on_batch(z_noise)\n",
    "\n",
    "#             y_real = np.ones(batch_size) - np.random.random_sample(batch_size) * 0.2\n",
    "#             y_fake = np.random.random_sample(batch_size) * 0.2\n",
    "\n",
    "#             dis_loss_real = dis_model.train_on_batch(image_batch, y_real)\n",
    "#             dis_loss_fake = dis_model.train_on_batch(generated_images, y_fake)\n",
    "#             d_loss = (dis_loss_real+dis_loss_fake)/2\n",
    "#             print(\"d_loss:\", d_loss)\n",
    "\n",
    "\n",
    "#             z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "#             g_loss = adversarial_model.train_on_batch(z_noise, [1] * batch_size)\n",
    "\n",
    "\n",
    "#             print(\"g_loss:\", g_loss)\n",
    "\n",
    "#             \"\"\"\n",
    "#             Save losses to Tensorboard after each epoch\n",
    "#             \"\"\"\n",
    "#             write_log(tensorboard, 'discriminator_loss', np.mean(d_loss), epoch)\n",
    "#             write_log(tensorboard, 'generator_loss', np.mean(g_loss), epoch)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             if epoch % 10 == 0:\n",
    "                \n",
    "#                 if epoch % 100 == 1:\n",
    "#                     path = \"/Volumes/LaMer/dl/modelsbak/facial_design_of_peking_opera/\"\n",
    "#                     #save models\n",
    "#                     # Specify the path for the generator model\n",
    "#                     gen_model.save(path + \"gen_model_\" + str(epoch) + \".h5\") \n",
    "\n",
    "#                     # Specify the path for the discriminator model\n",
    "#                     dis_model.save(path + \"dis_model_\" + str(epoch) + \".h5\") \n",
    "\n",
    "\n",
    "#                 z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "#                 gen_images1 = gen_model.predict_on_batch(z_noise)\n",
    "#                 index = 0\n",
    "#                 for img in gen_images1[:3]:\n",
    "# #                     save_rgb_img(img, \"results/one_{}.jpg\".format(epoch))\n",
    "                    \n",
    "#                     imsave('results/img_{}_'.format(epoch) + str(index) + '.jpg',img)\n",
    "#                     index += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Specify the path for the generator model\n",
    "#     gen_model.save(\"./gen_model.h5\") \n",
    "\n",
    "#     # Specify the path for the discriminator model\n",
    "#     dis_model.save(\"./dis_model.h5\") \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def predict(number=16):\n",
    "    path = \"data/models/0429/\"\n",
    "    filename = \"gen_model.h5\"\n",
    "    model = load_model(path + filename)\n",
    "    for i in range(number):\n",
    "        z_shape = 100\n",
    "        batch_size = 32\n",
    "        #z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "\n",
    "        z_noise = np.random.logistic(0, 1, size=(batch_size, z_shape))\n",
    "\n",
    "        gen_images1 = model.predict_on_batch(z_noise)\n",
    "        index = 0\n",
    "        for img in gen_images1:\n",
    "            imsave(path + 'predict/' + 'predict_img_{}_{}_'.format(i,index) + '.jpg',img)\n",
    "            index += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:299: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
