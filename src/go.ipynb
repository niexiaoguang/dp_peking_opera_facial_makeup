{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import io\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras import Sequential, Input, Model\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing import image\n",
    "from scipy.misc import imread, imsave\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# K.set_image_dim_ordering('tf')\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "def write_log(callback, name, loss, batch_no):\n",
    "    \"\"\"\n",
    "    Write training summary to TensorBoard\n",
    "    \"\"\"\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = loss\n",
    "    summary_value.tag = name\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()\n",
    "\n",
    "    \n",
    "    \n",
    "def build_generator():\n",
    "    gen_model = Sequential()\n",
    "\n",
    "    gen_model.add(Dense(input_dim=100, output_dim=2048))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "\n",
    "    gen_model.add(Dense(256 * 8 * 8))\n",
    "    gen_model.add(BatchNormalization())\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "    # 8x8 \n",
    "    gen_model.add(Reshape((8, 8, 256), input_shape=(256 * 8 * 8,)))\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "    # 16x16\n",
    "    gen_model.add(Conv2D(128, (5, 5), padding='same'))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "    # 32x32\n",
    "    gen_model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "\n",
    "    # 64x64\n",
    "    gen_model.add(Conv2D(32, (5, 5), padding='same'))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "#     gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "\n",
    "#     # 128x128\n",
    "    gen_model.add(Conv2D(3, (5, 5), padding='same'))\n",
    "    gen_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "\n",
    "    return gen_model\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "\n",
    "\n",
    "    dis_model = Sequential()\n",
    "\n",
    "    w = 64\n",
    "    h = 64\n",
    "    dis_model.add(\n",
    "        Conv2D(128, (5, 5),\n",
    "               padding='same',\n",
    "               input_shape=(w, h, 3))\n",
    "    )\n",
    "\n",
    "\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "    dis_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 32x32\n",
    "    dis_model.add(Conv2D(256, (3, 3)))\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "    dis_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 16x16\n",
    "    dis_model.add(Conv2D(512, (3, 3)))\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "    dis_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # 8x8\n",
    "    dis_model.add(Flatten())\n",
    "    dis_model.add(Dense(1024))\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    dis_model.add(Dense(1))\n",
    "    dis_model.add(Activation('sigmoid'))\n",
    "\n",
    "    return dis_model\n",
    "\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "\n",
    "# ===============================================================\n",
    "\n",
    "rawImagesPath = \"rawimages\"\n",
    "readyImagesPath = \"../data/readyimages64x64\"\n",
    "\n",
    "# ===============================================================\n",
    "\n",
    "# ===============================================================\n",
    "\n",
    "\n",
    "\n",
    "def pre_process_img(input_path, output_path):\n",
    "\n",
    "    w = 64\n",
    "    h = 64\n",
    "    files = os.listdir(rawImagesPath)\n",
    "    os.chdir(input_path)\n",
    "    if(not os.path.exists(output_path)):\n",
    "        os.makedirs(output_path)\n",
    "    for file in files:\n",
    "        if(os.path.isfile(file) & file.endswith('.jpg')):\n",
    "            img = Image.open(file)\n",
    "            img = img.resize((w, h), Image.ANTIALIAS)\n",
    "            img.save(os.path.join(readyImagesPath,file))\n",
    "\n",
    "\n",
    "\n",
    "def post_process_img(img):\n",
    "    res = img\n",
    "    return res\n",
    "\n",
    "def train():\n",
    "    batch_size = 64\n",
    "    z_shape = 100\n",
    "    epochs = 10000\n",
    "    dis_learning_rate = 0.0005\n",
    "    gen_learning_rate = 0.0004\n",
    "    dis_momentum = 0.9\n",
    "    gen_momentum = 0.9\n",
    "    dis_nesterov = True\n",
    "    gen_nesterov = True\n",
    "    \n",
    "    timestamp = int(time.time())\n",
    "    res_path = \"../data/results/\" + str(timestamp) + \"/\"\n",
    "\n",
    "    \n",
    "    if not os.path.exists(res_path):\n",
    "        os.mkdir(res_path)\n",
    "\n",
    "\n",
    "    \n",
    "    parameters_txt = \"batch_size:\" + str(batch_size) + \",\" + \\\n",
    "                    \"z_shape:\" + str(z_shape) + \",\" + \\\n",
    "                    \"epochs:\" + str(epochs) + \",\" + \\\n",
    "                    \"dis_learning_rate:\" + str(dis_learning_rate) + \",\" + \\\n",
    "                    \"gen_learning_rate:\" + str(gen_learning_rate) + \",\" + \\\n",
    "                    \"dis_momentum:\" + str(dis_momentum) + \",\" + \\\n",
    "                    \"gen_momentum\" + str(gen_momentum) + \",\" + \\\n",
    "                    \"dis_nesterov\" + str(dis_nesterov) + \",\" + \\\n",
    "                    \"gen_nesterov\" + str(gen_nesterov) + \",\" + \\\n",
    "                    \"END\"\n",
    "                    \n",
    "\n",
    "    parameters_file_name = \"parameters_\" + str(timestamp) + \".txt\"\n",
    "    with open(res_path + parameters_file_name, \"w\", encoding='utf-8') as f:\n",
    "        f.write(parameters_txt)\n",
    "        \n",
    "\n",
    "    # Loading images\n",
    "    all_images = []\n",
    "    for index, filename in enumerate(glob.glob(readyImagesPath + '/*.jpg')): \n",
    "        image = imread(filename, flatten=False, mode='RGB')\n",
    "        all_images.append(image)\n",
    "\n",
    "    # Convert to Numpy ndarray\n",
    "    X = np.array(all_images)\n",
    "    X = (X - 127.5) / 127.5\n",
    "\n",
    "\n",
    "    # Define optimizers\n",
    "    dis_optimizer = SGD(lr=dis_learning_rate, momentum=dis_momentum, nesterov=dis_nesterov)\n",
    "    gen_optimizer = SGD(lr=gen_learning_rate, momentum=gen_momentum, nesterov=gen_nesterov)\n",
    "\n",
    "\n",
    "    gen_model = build_generator()\n",
    "    gen_model.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "\n",
    "\n",
    "    dis_model = build_discriminator()\n",
    "    dis_model.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    adversarial_model = Sequential()\n",
    "    adversarial_model.add(gen_model)\n",
    "    dis_model.trainable = False\n",
    "    adversarial_model.add(dis_model)\n",
    "\n",
    "\n",
    "    adversarial_model.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "\n",
    "    log_path = \"../data/logs/\" + str(timestamp)\n",
    "    tensorboard = TensorBoard(log_dir=log_path, write_images=True, write_grads=True,\n",
    "                                  write_graph=True)\n",
    "\n",
    "    tensorboard.set_model(gen_model)\n",
    "    tensorboard.set_model(dis_model)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        number_of_batches = int(X.shape[0] / batch_size)\n",
    "        print(\"Number of batches\", number_of_batches)\n",
    "        for index in range(number_of_batches):\n",
    "\n",
    "\n",
    "            z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "\n",
    "\n",
    "            image_batch = X[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "            generated_images = gen_model.predict_on_batch(z_noise)\n",
    "\n",
    "            y_real = np.ones(batch_size) - np.random.random_sample(batch_size) * 0.2\n",
    "            y_fake = np.random.random_sample(batch_size) * 0.2\n",
    "\n",
    "            dis_loss_real = dis_model.train_on_batch(image_batch, y_real)\n",
    "            dis_loss_fake = dis_model.train_on_batch(generated_images, y_fake)\n",
    "            d_loss = (dis_loss_real+dis_loss_fake)/2\n",
    "            print(\"d_loss:\", d_loss)\n",
    "\n",
    "\n",
    "            z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "            g_loss = adversarial_model.train_on_batch(z_noise, [1] * batch_size)\n",
    "\n",
    "\n",
    "            print(\"g_loss:\", g_loss)\n",
    "\n",
    "            \"\"\"\n",
    "            Save losses to Tensorboard after each epoch\n",
    "            \"\"\"\n",
    "            write_log(tensorboard, 'discriminator_loss', np.mean(d_loss), epoch)\n",
    "            write_log(tensorboard, 'generator_loss', np.mean(g_loss), epoch)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                \n",
    "#                 if epoch % 100 == 1:\n",
    "#                     path = \"/Volumes/LaMer/dl/modelsbak/facial_design_of_peking_opera/\"\n",
    "#                     #save models\n",
    "#                     # Specify the path for the generator model\n",
    "#                     gen_model.save(path + \"gen_model_\" + str(epoch) + \".h5\") \n",
    "\n",
    "#                     # Specify the path for the discriminator model\n",
    "#                     dis_model.save(path + \"dis_model_\" + str(epoch) + \".h5\") \n",
    "\n",
    "\n",
    "                z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "                gen_images1 = gen_model.predict_on_batch(z_noise)\n",
    "                index = 0\n",
    "                for img in gen_images1[:3]:\n",
    "#                     save_rgb_img(img, \"results/one_{}.jpg\".format(epoch))\n",
    "                    \n",
    "                    imsave(res_path + 'img_{}_'.format(epoch) + str(index) + '.jpg',img)\n",
    "                    index += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Specify the path for the generator model\n",
    "    gen_model.save(res_path + \"gen_model.h5\") \n",
    "\n",
    "    # Specify the path for the discriminator model\n",
    "    dis_model.save(res_path + \"dis_model.h5\") \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def predict(number=16):\n",
    "    path = \"data/models/0429/\"\n",
    "    filename = \"gen_model.h5\"\n",
    "    model = load_model(path + filename)\n",
    "    for i in range(number):\n",
    "        z_shape = 100\n",
    "        batch_size = 32\n",
    "        #z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "\n",
    "        z_noise = np.random.logistic(0, 1, size=(batch_size, z_shape))\n",
    "\n",
    "        gen_images1 = model.predict_on_batch(z_noise)\n",
    "        index = 0\n",
    "        for img in gen_images1:\n",
    "            imsave(path + 'predict/' + 'predict_img_{}_{}_'.format(i,index) + '.jpg',img)\n",
    "            index += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:208: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=2048)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "Number of batches 5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss: 0.728821337223053\n",
      "g_loss: 0.7123342\n",
      "d_loss: 0.7059892416000366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:305: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_loss: 0.69448876\n",
      "d_loss: 0.6907384991645813\n",
      "g_loss: 0.6753411\n",
      "d_loss: 0.6736881732940674\n",
      "g_loss: 0.65562993\n",
      "d_loss: 0.6583507061004639\n",
      "g_loss: 0.63687444\n",
      "Epoch is 1\n",
      "Number of batches 5\n",
      "d_loss: 0.648788332939148\n",
      "g_loss: 0.6192946\n",
      "d_loss: 0.6414546966552734\n",
      "g_loss: 0.6037532\n",
      "d_loss: 0.6348379254341125\n",
      "g_loss: 0.58903587\n",
      "d_loss: 0.628447413444519\n",
      "g_loss: 0.57699144\n",
      "d_loss: 0.6257262229919434\n",
      "g_loss: 0.5670835\n",
      "Epoch is 2\n",
      "Number of batches 5\n",
      "d_loss: 0.6250544786453247\n",
      "g_loss: 0.5618547\n",
      "d_loss: 0.6220107674598694\n",
      "g_loss: 0.5602977\n",
      "d_loss: 0.6190003156661987\n",
      "g_loss: 0.55752504\n",
      "d_loss: 0.6231516599655151\n",
      "g_loss: 0.5616774\n",
      "d_loss: 0.619745135307312\n",
      "g_loss: 0.5663461\n",
      "Epoch is 3\n",
      "Number of batches 5\n",
      "d_loss: 0.6302335262298584\n",
      "g_loss: 0.56645966\n",
      "d_loss: 0.6361082792282104\n",
      "g_loss: 0.5509863\n",
      "d_loss: 0.6533790826797485\n",
      "g_loss: 0.5292435\n",
      "d_loss: 0.6822720766067505\n",
      "g_loss: 0.5098835\n",
      "d_loss: 0.6935012936592102\n",
      "g_loss: 0.5125808\n",
      "Epoch is 4\n",
      "Number of batches 5\n",
      "d_loss: 0.6910113096237183\n",
      "g_loss: 0.5498301\n",
      "d_loss: 0.6671016216278076\n",
      "g_loss: 0.62702644\n",
      "d_loss: 0.634594202041626\n",
      "g_loss: 0.754007\n",
      "d_loss: 0.5799369812011719\n",
      "g_loss: 0.92195606\n",
      "d_loss: 0.5520076751708984\n",
      "g_loss: 1.1023341\n",
      "Epoch is 5\n",
      "Number of batches 5\n",
      "d_loss: 0.5239617824554443\n",
      "g_loss: 1.2587427\n",
      "d_loss: 0.5107084512710571\n",
      "g_loss: 1.3577707\n",
      "d_loss: 0.506261944770813\n",
      "g_loss: 1.3880198\n",
      "d_loss: 0.5051469802856445\n",
      "g_loss: 1.344718\n",
      "d_loss: 0.5011721849441528\n",
      "g_loss: 1.2549524\n",
      "Epoch is 6\n",
      "Number of batches 5\n",
      "d_loss: 0.49294406175613403\n",
      "g_loss: 1.1372714\n",
      "d_loss: 0.5213220715522766\n",
      "g_loss: 1.0002083\n",
      "d_loss: 0.5414072275161743\n",
      "g_loss: 0.83801186\n",
      "d_loss: 0.5880236625671387\n",
      "g_loss: 0.6747028\n",
      "d_loss: 0.6280734539031982\n",
      "g_loss: 0.5834198\n",
      "Epoch is 7\n",
      "Number of batches 5\n",
      "d_loss: 0.6189713478088379\n",
      "g_loss: 0.5586558\n",
      "d_loss: 0.6292753219604492\n",
      "g_loss: 0.5480391\n",
      "d_loss: 0.6153550148010254\n",
      "g_loss: 0.5372824\n",
      "d_loss: 0.6302992105484009\n",
      "g_loss: 0.5329732\n",
      "d_loss: 0.6270633935928345\n",
      "g_loss: 0.52916646\n",
      "Epoch is 8\n",
      "Number of batches 5\n",
      "d_loss: 0.622413694858551\n",
      "g_loss: 0.5307211\n",
      "d_loss: 0.6386960744857788\n",
      "g_loss: 0.5336556\n",
      "d_loss: 0.6173989772796631\n",
      "g_loss: 0.54114985\n",
      "d_loss: 0.6112145781517029\n",
      "g_loss: 0.55108225\n",
      "d_loss: 0.605722188949585\n",
      "g_loss: 0.56336373\n",
      "Epoch is 9\n",
      "Number of batches 5\n",
      "d_loss: 0.5878987312316895\n",
      "g_loss: 0.57805526\n",
      "d_loss: 0.5898356437683105\n",
      "g_loss: 0.5936761\n",
      "d_loss: 0.5774796009063721\n",
      "g_loss: 0.61159325\n",
      "d_loss: 0.5650319457054138\n",
      "g_loss: 0.6275022\n",
      "d_loss: 0.5672492980957031\n",
      "g_loss: 0.64247465\n",
      "Epoch is 10\n",
      "Number of batches 5\n",
      "d_loss: 0.5560069680213928\n",
      "g_loss: 0.6552447\n",
      "d_loss: 0.5486448407173157\n",
      "g_loss: 0.6649785\n",
      "d_loss: 0.5448066592216492\n",
      "g_loss: 0.67372644\n",
      "d_loss: 0.5473097562789917\n",
      "g_loss: 0.6797812\n",
      "d_loss: 0.5427002906799316\n",
      "g_loss: 0.6881093\n",
      "Epoch is 11\n",
      "Number of batches 5\n",
      "d_loss: 0.5406366586685181\n",
      "g_loss: 0.6991551\n",
      "d_loss: 0.529474139213562\n",
      "g_loss: 0.71144235\n",
      "d_loss: 0.5131668448448181\n",
      "g_loss: 0.7254342\n",
      "d_loss: 0.5174159407615662\n",
      "g_loss: 0.74184835\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-244a269e0def>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0my_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mdis_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mdis_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdis_loss_real\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdis_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
